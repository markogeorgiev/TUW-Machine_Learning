{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MLP for Airline Passenger Satisfaction Dataset ",
   "id": "80ddfe36189e7fc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing the Data",
   "id": "e82926cb5312801b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:32:12.496322Z",
     "start_time": "2025-04-24T17:32:12.224262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load and merge datasets\n",
    "df_train = pd.read_csv('all_datasets/Airline_passenger_satiscation_train.csv')\n",
    "df_test = pd.read_csv('all_datasets/Airline_passenger_satiscation_test.csv')\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df = df.drop(columns=[\"id\"])"
   ],
   "id": "ad6e261989373e82",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:32:13.453448Z",
     "start_time": "2025-04-24T17:32:13.379506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Label encode categorical columns\n",
    "categorical_cols = [\"Gender\", \"Customer Type\", \"Type of Travel\", \"Class\"]\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le"
   ],
   "id": "8f2f5fd65d01d1c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:32:13.667072Z",
     "start_time": "2025-04-24T17:32:13.647424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_encoder = LabelEncoder()\n",
    "df[\"satisfaction\"] = target_encoder.fit_transform(df[\"satisfaction\"])"
   ],
   "id": "954feb31916728d2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:32:15.557797Z",
     "start_time": "2025-04-24T17:32:15.532419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fill missing values\n",
    "df = df.fillna(0)"
   ],
   "id": "fc60972fea35f18e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:32:17.488924Z",
     "start_time": "2025-04-24T17:32:17.461342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature/target split\n",
    "X = df.drop(columns=[\"satisfaction\"]).values\n",
    "y = df[\"satisfaction\"].values"
   ],
   "id": "ba33ff4505752614",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:32:17.688688Z",
     "start_time": "2025-04-24T17:32:17.628483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ],
   "id": "8d6234d77d78c7ee",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:32:17.938755Z",
     "start_time": "2025-04-24T17:32:17.934308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "split_ratios = [0.5, 0.3, 0.2, 0.1]"
   ],
   "id": "81a091204ed5b965",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:56:38.105335Z",
     "start_time": "2025-04-24T17:49:37.415488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for split in split_ratios:\n",
    "    print(f\"\\n--- Evaluating {int((1 - split) * 100)}/{int(split * 100)} Train/Test Split ---\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=42, stratify=y)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = len(np.unique(y))\n",
    "    model = MLP(input_dim, output_dim)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "    # Train model\n",
    "    epochs = 300\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = loss_fn(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.argmax(model(X_test_tensor), dim=1).numpy()\n",
    "        y_true = y_test_tensor.numpy()\n",
    "\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        prec = precision_score(y_true, preds, zero_division=0)\n",
    "        rec = recall_score(y_true, preds, zero_division=0)\n",
    "        f1 = f1_score(y_true, preds, zero_division=0)\n",
    "\n",
    "        print(f\"Accuracy:  {acc:.4f}\")\n",
    "        print(f\"Precision: {prec:.4f}\")\n",
    "        print(f\"Recall:    {rec:.4f}\")\n",
    "        print(f\"F1 Score:  {f1:.4f}\")\n"
   ],
   "id": "802c31d5847abeeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating 50/50 Train/Test Split ---\n",
      "Accuracy:  0.9590\n",
      "Precision: 0.9710\n",
      "Recall:    0.9336\n",
      "F1 Score:  0.9519\n",
      "\n",
      "--- Evaluating 70/30 Train/Test Split ---\n",
      "Accuracy:  0.9606\n",
      "Precision: 0.9693\n",
      "Recall:    0.9391\n",
      "F1 Score:  0.9539\n",
      "\n",
      "--- Evaluating 80/20 Train/Test Split ---\n",
      "Accuracy:  0.9608\n",
      "Precision: 0.9684\n",
      "Recall:    0.9404\n",
      "F1 Score:  0.9542\n",
      "\n",
      "--- Evaluating 90/10 Train/Test Split ---\n",
      "Accuracy:  0.9601\n",
      "Precision: 0.9698\n",
      "Recall:    0.9374\n",
      "F1 Score:  0.9533\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing out different MLPs",
   "id": "a1847aac82818a64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:59:37.484068Z",
     "start_time": "2025-04-24T18:45:00.051219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load and merge datasets\n",
    "df_train = pd.read_csv('all_datasets/Airline_passenger_satiscation_train.csv')\n",
    "df_test = pd.read_csv('all_datasets/Airline_passenger_satiscation_test.csv')\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df = df.drop(columns=[\"id\"])\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = [\"Gender\", \"Customer Type\", \"Type of Travel\", \"Class\"]\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Encode target\n",
    "target_encoder = LabelEncoder()\n",
    "df[\"satisfaction\"] = target_encoder.fit_transform(df[\"satisfaction\"])\n",
    "\n",
    "# Fill missing values and prepare data\n",
    "df = df.fillna(0)\n",
    "X = df.drop(columns=[\"satisfaction\"]).values\n",
    "y = df[\"satisfaction\"].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Fixed 70/30 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define MLP architectures\n",
    "input_dim = X.shape[1]\n",
    "output_dim = len(np.unique(y))\n",
    "\n",
    "model_architectures = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(input_dim, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, output_dim),\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(input_dim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, output_dim),\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(input_dim, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, output_dim),\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(input_dim, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, output_dim),\n",
    "    )\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for i, architecture in enumerate(model_architectures, 1):\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.model = architecture\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "\n",
    "    model = MLP()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(300):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = loss_fn(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.argmax(model(X_test_tensor), dim=1).numpy()\n",
    "        y_true = y_test_tensor.numpy()\n",
    "\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        prec = precision_score(y_true, preds, zero_division=0)\n",
    "        rec = recall_score(y_true, preds, zero_division=0)\n",
    "        f1 = f1_score(y_true, preds, zero_division=0)\n",
    "\n",
    "        results.append((f\"MLP{i}\", acc, prec, rec, f1))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMODEL\\tAccuracy\\tPrecision\\tRecall\\t\\tF1-Score\")\n",
    "for model_name, acc, prec, rec, f1 in results:\n",
    "    print(f\"{model_name}\\t{acc:.4f}\\t\\t{prec:.4f}\\t\\t{rec:.4f}\\t\\t{f1:.4f}\")"
   ],
   "id": "ae6bf7cf8f4d9b8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL\tAccuracy\tPrecision\tRecall\t\tF1-Score\n",
      "MLP1\t0.9511\t\t0.9554\t\t0.9309\t\t0.9430\n",
      "MLP2\t0.9575\t\t0.9675\t\t0.9336\t\t0.9502\n",
      "MLP3\t0.9571\t\t0.9570\t\t0.9437\t\t0.9503\n",
      "MLP4\t0.9630\t\t0.9724\t\t0.9415\t\t0.9567\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cross Validation\n",
    "using the best model from the previous section"
   ],
   "id": "e0eb0bd351e10e64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T21:49:26.772339Z",
     "start_time": "2025-04-24T20:37:09.371354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df_train = pd.read_csv('all_datasets/Airline_passenger_satiscation_train.csv')\n",
    "df_test = pd.read_csv('all_datasets/Airline_passenger_satiscation_test.csv')\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df = df.drop(columns=[\"id\"])\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = [\"Gender\", \"Customer Type\", \"Type of Travel\", \"Class\"]\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Encode target\n",
    "target_encoder = LabelEncoder()\n",
    "df[\"satisfaction\"] = target_encoder.fit_transform(df[\"satisfaction\"])\n",
    "\n",
    "# Prepare data\n",
    "df = df.fillna(0)\n",
    "X = df.drop(columns=[\"satisfaction\"]).values\n",
    "y = df[\"satisfaction\"].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Custom MLP with specified architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    # Cross-validation function (CPU only)\n",
    "    def run_cv(X, y, n_splits):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        input_dim = X.shape[1]\n",
    "        output_dim = len(np.unique(y))\n",
    "        results = []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "            print(f\"\\n[Fold {fold}/{n_splits}]\")\n",
    "\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "            y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "            model = MLP(input_dim, output_dim)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "            for epoch in range(300):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_train_tensor)\n",
    "                loss = loss_fn(outputs, y_train_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(X_test_tensor).argmax(dim=1).numpy()\n",
    "                y_true = y_test_tensor.numpy()\n",
    "\n",
    "                acc = accuracy_score(y_true, preds)\n",
    "                prec = precision_score(y_true, preds, zero_division=0)\n",
    "                rec = recall_score(y_true, preds, zero_division=0)\n",
    "                f1 = f1_score(y_true, preds, zero_division=0)\n",
    "\n",
    "                results.append((acc, prec, rec, f1))\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Run 5-fold and 10-fold CV\n",
    "    results_5 = run_cv(X, y, 5)\n",
    "    results_10 = run_cv(X, y, 10)\n",
    "\n",
    "    # Print results\n",
    "    def print_results(results, label):\n",
    "        accs, precs, recs, f1s = zip(*results)\n",
    "        print(f\"\\n=== {label} Results ===\")\n",
    "        print(\"MODEL\\tAccuracy\\tPrecision\\tRecall\\t\\tF1-Score\")\n",
    "        print(f\"{label}\\t{np.mean(accs):.4f}\\t\\t{np.mean(precs):.4f}\\t\\t{np.mean(recs):.4f}\\t\\t{np.mean(f1s):.4f}\")\n",
    "\n",
    "    print_results(results_5, \"5-Fold\")\n",
    "    print_results(results_10, \"10-Fold\")"
   ],
   "id": "2b43327b6796e03e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 1/5]\n",
      "\n",
      "[Fold 2/5]\n",
      "\n",
      "[Fold 3/5]\n",
      "\n",
      "[Fold 4/5]\n",
      "\n",
      "[Fold 5/5]\n",
      "\n",
      "[Fold 1/10]\n",
      "\n",
      "[Fold 2/10]\n",
      "\n",
      "[Fold 3/10]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 37\u001B[39m\n\u001B[32m     33\u001B[39m X = scaler.fit_transform(X)\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# Custom MLP with specified architecture\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m \u001B[38;5;28;43;01mclass\u001B[39;49;00m\u001B[38;5;250;43m \u001B[39;49m\u001B[34;43;01mMLP\u001B[39;49;00m\u001B[43m(\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mModule\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[38;5;250;43m \u001B[39;49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 100\u001B[39m, in \u001B[36mMLP\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     98\u001B[39m \u001B[38;5;66;03m# Run 5-fold and 10-fold CV\u001B[39;00m\n\u001B[32m     99\u001B[39m results_5 = run_cv(X, y, \u001B[32m5\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m results_10 = \u001B[43mrun_cv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    102\u001B[39m \u001B[38;5;66;03m# Print results\u001B[39;00m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprint_results\u001B[39m(results, label):\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 81\u001B[39m, in \u001B[36mMLP.run_cv\u001B[39m\u001B[34m(X, y, n_splits)\u001B[39m\n\u001B[32m     79\u001B[39m     outputs = model(X_train_tensor)\n\u001B[32m     80\u001B[39m     loss = loss_fn(outputs, y_train_tensor)\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     82\u001B[39m     optimizer.step()\n\u001B[32m     84\u001B[39m model.eval()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\TU_Wien\\Machine_Learning\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    618\u001B[39m         Tensor.backward,\n\u001B[32m    619\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m         inputs=inputs,\n\u001B[32m    625\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\TU_Wien\\Machine_Learning\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\TU_Wien\\Machine_Learning\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    824\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    825\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "=== 5-Fold Results ===\n",
    "MODEL    Accuracy    Precision    Recall        F1-Score\n",
    "5-Fold    0.9648        0.9750        0.9433        0.9589\n",
    "\n",
    "=== 10-Fold Results ===\n",
    "MODEL    Accuracy    Precision    Recall        F1-Score\n",
    "10-Fold    "
   ],
   "id": "4c5a494406b261a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "75b4aa309dbe157"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
